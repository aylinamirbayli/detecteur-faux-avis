# 2) DTM du nouveau texte avec dictionnaire de référence
  dict_terms <- Terms(dtm_ref)
  dtm_new <- DocumentTermMatrix(corp_new, control = list(dictionary = dict_terms))

  # Cas texte trop court / aucun terme reconnu
  if (ncol(dtm_new) == 0) {
    return(list(
      proba_fake = NA_real_,
      prediction = "Texte trop court ou pas assez de mots utiles"
    ))
  }

  # 3) Passage en data.frame
  x_new <- as.data.frame(as.matrix(dtm_new), check.names = FALSE)

  # 4) Aligner exactement les colonnes attendues par le modèle
  missing_cols <- setdiff(dict_terms, colnames(x_new))
  if (length(missing_cols) > 0) {
    x_new[missing_cols] <- 0
  }
  x_new <- x_new[, dict_terms, drop = FALSE]

  # 5) Proba + classe
  proba_fake <- predict(modele, newdata = x_new, type = "response")
  proba_fake <- as.numeric(proba_fake)

  classe <- ifelse(proba_fake >= seuil, 1, 0)

  list(
    proba_fake = proba_fake,
    prediction = ifelse(classe == 1, "FAUX (OR)", "VRAI (CG)")
  )
}

### 8.1 Test de la fonction de prédiction
set.seed(1)
ex_vrai <- data[data$label == "CG", "text_"][1]
ex_faux <- data[data$label == "OR", "text_"][1]

cat("=== Test avec seuil standard (0.5) ===\n")
predire_avis(ex_vrai, modele_logit, dtm2, seuil = 0.5)
predire_avis(ex_faux, modele_logit, dtm2, seuil = 0.5)

cat("\n=== Test avec seuil optimisé (0.85) ===\n")
predire_avis(ex_vrai, modele_logit, dtm2, seuil = 0.85)
predire_avis(ex_faux, modele_logit, dtm2, seuil = 0.85)

Le modèle renvoie une probabilité d'être faux (OR). Le seuil de classification détermine à partir de quelle probabilité un avis est considéré comme frauduleux.

Avec un seuil standard de 0.5 :
- L'avis authentique (CG) a une probabilité de 0,15 → classé comme VRAI ✓
- L'avis frauduleux (OR) a une probabilité de 0,77 → classé comme FAUX ✓

Avec un seuil optimisé de 0.85 :
- L'avis authentique (CG) conserve sa probabilité de 0,15 → classé comme VRAI ✓
- L'avis frauduleux (OR) avec sa probabilité de 0,77 est maintenant classé comme VRAI (car 0,77 < 0,85)

Cette différence illustre l'impact du choix du seuil : un seuil plus élevé réduit les faux positifs (avis authentiques classés à tort comme faux) au prix de laisser passer certains avis frauduleux dont la probabilité est modérée (entre 0,5 et 0,85).

## 9. Interprétation du modèle : analyse des coefficients
# Coefficients du modèle
coef_mod <- coef(modele_logit)

# On enlève l'intercept
coef_mod <- coef_mod[names(coef_mod) != "(Intercept)"]

# Top mots qui augmentent la proba d'être FAUX (OR)
top_fake <- sort(coef_mod, decreasing = TRUE)[1:20]

# Top mots qui augmentent la proba d'être VRAI (CG) (coeff négatifs)
top_vrai <- sort(coef_mod, decreasing = FALSE)[1:20]

top_fake
top_vrai

Les coefficients montrent quels mots poussent le modèle vers FAUX (OR) ou VRAI (CG) :
coefficients positifs → augmentent la probabilité d'un avis faux (mots plutôt vagues/nuancés comme maybe, however, instead).
coefficients négatifs → augmentent la probabilité d'un avis authentique (mots plus concrets liés au produit comme sturdy, instructions, plastic).
Donc le modèle distingue surtout vocabulaire générique vs détails spécifiques.

### 9.1 Test sur un nouvel avis
texte_test <- "This product is amazing, I recommend it to everyone!!!"
predire_avis(texte_test, modele_logit, dtm2)

## 10. Conclusion

Dans ce projet, nous avons cherché à déterminer s'il est possible de détecter automatiquement les avis faux à partir du contenu textuel.

À l'aide de techniques de traitement automatique du langage (NLP) et d'un modèle de régression logistique, nous avons transformé les avis en variables numériques (matrice documents-termes), puis entraîné un classifieur capable de distinguer avis authentiques (CG) et avis faux (OR).

Les résultats montrent que le modèle obtient de bonnes performances (accuracy ≈ 82 %, F1-score ≈ 0,82), ce qui indique qu'une grande partie des avis peut être correctement classée uniquement à partir du texte.


L'analyse des coefficients met en évidence que les avis faux utilisent davantage un vocabulaire vague et générique, tandis que les avis authentiques contiennent plus de détails concrets liés au produit.

### Optimisation du seuil de décision

Bien que le seuil standard soit de 0.5 pour l'évaluation du modèle, nous avons choisi d'utiliser un seuil de 0.85 dans l'application finale pour les raisons suivantes :

1. Réduction des faux positifs : Il est préférable de laisser passer quelques avis frauduleux plutôt que de sanctionner à tort des avis authentiques, ce qui aurait un impact négatif sur la confiance des utilisateurs et pourrait nuire injustement à des clients honnêtes.

2. Compromis précision/rappel : Un seuil plus élevé améliore la précision au détriment du rappel, ce qui est approprié dans un contexte de modération d'avis où la priorité est d'éviter les erreurs de classification d'avis légitimes.

3. Flexibilité : L'application Shiny permet à l'utilisateur d'ajuster le seuil selon ses besoins via un curseur interactif, offrant ainsi une solution adaptable à différents contextes d'utilisation.

Ce choix illustre l'importance d'adapter les modèles de machine learning aux contraintes métier et aux coûts relatifs des différents types d'erreurs, plutôt que de se limiter aux valeurs par défaut.

### Perspectives

Ce projet illustre ainsi comment des méthodes simples de NLP peuvent contribuer à la détection automatique de faux avis avec des performances satisfaisantes. Plusieurs améliorations pourraient être envisagées pour renforcer le modèle : l'utilisation de pondérations TF-IDF, l'exploration de modèles plus complexes (Random Forest, réseaux de neurones), l'ajout de features textuelles supplémentaires (sentiment, longueur), et l'adaptation du système à d'autres langues comme le français.

## 11. Sauvegarde des modèles
# Créer le dossier results s'il n'existe pas
if (!dir.exists("results")) {
  dir.create("results")
  print("Dossier 'results' créé avec succès")
}

# Sauvegarder les modèles
saveRDS(modele_logit, "results/modele_logit.rds")
saveRDS(dtm2, "results/dtm_ref.rds")

print("Modèles sauvegardés avec succès !")
